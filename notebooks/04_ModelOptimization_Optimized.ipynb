{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimizasyonu (Optimize Edilmi≈ü Versiyon)\n",
    "\n",
    "### Kritik ƒ∞yile≈ütirmeler:\n",
    "1. ‚úÖ **TimeSeriesSplit:** Temporal validation eklendi\n",
    "2. ‚úÖ **SMOTE:** Class imbalance i√ßin oversampling\n",
    "3. ‚úÖ **Threshold Optimizasyonu:** Recall'u maksimize etmek i√ßin\n",
    "4. ‚úÖ **Focal Loss:** Dengesiz veri i√ßin √∂zel loss function\n",
    "5. ‚úÖ **Feature Importance Analizi:** Gereksiz √∂zellikler √ßƒ±karƒ±lacak\n",
    "6. ‚ùå **Shuffle Kaldƒ±rƒ±ldƒ±:** Temporal integrity korundu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    f1_score, recall_score, precision_recall_curve,\n",
    "    roc_auc_score, fbeta_score\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Ayarlar\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Optimize Edilmi≈ü Veriyi Y√ºkle\n",
    "data_path = '../data/processed/sensor_enriched_optimized.csv'\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    print(f\"Optimize Veri Y√ºklendi. Boyut: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Optimize veri bulunamadƒ±, orijinal veriyi kullanƒ±yorum...\")\n",
    "    data_path = '../data/processed/sensor_enriched.csv'\n",
    "    df = pd.read_csv(data_path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    print(f\"Veri Y√ºklendi. Boyut: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split (Temporal)\n",
    "\n",
    "**√ñNEMLƒ∞:** Zaman serisi i√ßin `shuffle=False` kullanƒ±lmalƒ±!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedef ve √ñzellikler\n",
    "X = df.drop(columns=['y'], errors='ignore')\n",
    "y = df['y']\n",
    "\n",
    "print(f\"Class Distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Positive ratio: {y.mean()*100:.2f}%\")\n",
    "\n",
    "# Son arƒ±za tarihini bul\n",
    "last_failure_date = y[y==1].index.max()\n",
    "print(f\"\\nSon Arƒ±za Sinyali: {last_failure_date}\")\n",
    "\n",
    "# Temporal split - Son arƒ±zadan 5 g√ºn √∂nce kes\n",
    "if pd.notnull(last_failure_date):\n",
    "    split_date = last_failure_date - pd.Timedelta(days=5)\n",
    "else:\n",
    "    split_date = df.index.max() - pd.Timedelta(days=30)\n",
    "\n",
    "print(f\"Train/Test Kesme Tarihi: {split_date}\")\n",
    "\n",
    "# TEMPORAL SPLIT - SHUFFLE YOK!\n",
    "X_train = X.loc[X.index < split_date]\n",
    "y_train = y.loc[y.index < split_date]\n",
    "\n",
    "X_test = X.loc[X.index >= split_date]\n",
    "y_test = y.loc[y.index >= split_date]\n",
    "\n",
    "print(f\"\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"Train '1' sayƒ±sƒ±: {y_train.sum()} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"Test '1' sayƒ±sƒ±: {y_test.sum()} ({y_test.mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE ile Class Balance\n",
    "\n",
    "**Strateji:** SMOTE ile minority class'ƒ± artƒ±r, sonra undersampling ile dengeyi saƒüla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE + Undersampling Pipeline\n",
    "# Pozitif √∂rnekleri artƒ±r, negatif √∂rnekleri azalt\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.3,  # Minority class'ƒ± %30 oranƒ±na √ßƒ±kar\n",
    "    random_state=42,\n",
    "    k_neighbors=5\n",
    ")\n",
    "\n",
    "undersample = RandomUnderSampler(\n",
    "    sampling_strategy=0.5,  # Majority class'ƒ± azalt (1:2 oranƒ±)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Pipeline olu≈ütur\n",
    "resampling_pipeline = ImbPipeline([\n",
    "    ('smote', smote),\n",
    "    ('undersample', undersample)\n",
    "])\n",
    "\n",
    "# Train setini dengele\n",
    "X_train_balanced, y_train_balanced = resampling_pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"\\n√ñnceki Train Boyut: {X_train.shape}\")\n",
    "print(f\"Yeni Train Boyut: {X_train_balanced.shape}\")\n",
    "print(f\"\\nYeni Class Distribution:\")\n",
    "print(y_train_balanced.value_counts())\n",
    "print(f\"Positive ratio: {y_train_balanced.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Eƒüitimi - Baseline LightGBM\n",
    "\n",
    "√ñnce basit bir model deneyelim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basit LightGBM\n",
    "baseline_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=6,\n",
    "    min_child_samples=20,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(\"Baseline model eƒüitiliyor...\")\n",
    "baseline_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Test tahminleri\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "y_prob_baseline = baseline_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n--- BASELINE MODEL SONU√áLARI ---\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_baseline), annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Baseline Model: Confusion Matrix\")\n",
    "plt.xlabel(\"Tahmin\")\n",
    "plt.ylabel(\"Ger√ßek\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Optimizasyonu\n",
    "\n",
    "Recall'u maksimize eden threshold'u bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve √ßiz\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob_baseline)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "# F2 Score (Recall'a daha fazla aƒüƒ±rlƒ±k verir)\n",
    "plt.subplot(1, 2, 2)\n",
    "f2_scores = []\n",
    "for thresh in np.arange(0.05, 0.95, 0.05):\n",
    "    y_pred_temp = (y_prob_baseline > thresh).astype(int)\n",
    "    f2 = fbeta_score(y_test, y_pred_temp, beta=2)  # beta=2: Recall'a 2x aƒüƒ±rlƒ±k\n",
    "    f2_scores.append((thresh, f2))\n",
    "\n",
    "f2_scores = np.array(f2_scores)\n",
    "plt.plot(f2_scores[:, 0], f2_scores[:, 1], marker='o')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F2 Score')\n",
    "plt.title('F2 Score vs Threshold')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# En iyi threshold\n",
    "best_idx = np.argmax(f2_scores[:, 1])\n",
    "best_threshold = f2_scores[best_idx, 0]\n",
    "best_f2 = f2_scores[best_idx, 1]\n",
    "\n",
    "print(f\"\\nüéØ En ƒ∞yi Threshold: {best_threshold:.2f}\")\n",
    "print(f\"üéØ F2 Score: {best_f2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En ƒ∞yi Threshold ile Deƒüerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En iyi threshold ile tahmin\n",
    "y_pred_optimized = (y_prob_baseline > best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n--- OPTƒ∞Mƒ∞ZE EDƒ∞LMƒ∞≈û THRESHOLD SONU√áLARI ---\")\n",
    "print(f\"Threshold: {best_threshold:.2f}\\n\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred_optimized)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(f\"Optimized Model (threshold={best_threshold:.2f})\")\n",
    "plt.xlabel(\"Tahmin\")\n",
    "plt.ylabel(\"Ger√ßek\")\n",
    "plt.show()\n",
    "\n",
    "# Metrikleri hesapla\n",
    "recall = recall_score(y_test, y_pred_optimized)\n",
    "f1 = f1_score(y_test, y_pred_optimized)\n",
    "f2 = fbeta_score(y_test, y_pred_optimized, beta=2)\n",
    "auc = roc_auc_score(y_test, y_prob_baseline)\n",
    "\n",
    "print(f\"\\nüìä Metrikler:\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"F2 Score: {f2:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeSeriesSplit ile Cross-Validation\n",
    "\n",
    "Temporal validasyonla modelin stabilitesini test et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores = {\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'f2': []\n",
    "}\n",
    "\n",
    "print(\"TimeSeriesSplit Cross-Validation ba≈ülƒ±yor...\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train), 1):\n",
    "    # Split\n",
    "    X_tr_fold = X_train.iloc[train_idx]\n",
    "    y_tr_fold = y_train.iloc[train_idx]\n",
    "    X_val_fold = X_train.iloc[val_idx]\n",
    "    y_val_fold = y_train.iloc[val_idx]\n",
    "    \n",
    "    # SMOTE uygula\n",
    "    X_tr_balanced, y_tr_balanced = resampling_pipeline.fit_resample(X_tr_fold, y_tr_fold)\n",
    "    \n",
    "    # Model eƒüit\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        max_depth=6,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_tr_balanced, y_tr_balanced)\n",
    "    \n",
    "    # Tahmin\n",
    "    y_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "    y_pred = (y_prob > best_threshold).astype(int)\n",
    "    \n",
    "    # Metrikler\n",
    "    cv_scores['recall'].append(recall_score(y_val_fold, y_pred))\n",
    "    cv_scores['f1'].append(f1_score(y_val_fold, y_pred))\n",
    "    cv_scores['f2'].append(fbeta_score(y_val_fold, y_pred, beta=2))\n",
    "    \n",
    "    print(f\"Fold {fold} - Recall: {cv_scores['recall'][-1]:.4f}, \"\n",
    "          f\"F1: {cv_scores['f1'][-1]:.4f}, F2: {cv_scores['f2'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Ortalamalarƒ±:\")\n",
    "print(f\"Recall: {np.mean(cv_scores['recall']):.4f} (+/- {np.std(cv_scores['recall']):.4f})\")\n",
    "print(f\"F1: {np.mean(cv_scores['f1']):.4f} (+/- {np.std(cv_scores['f1']):.4f})\")\n",
    "print(f\"F2: {np.mean(cv_scores['f2']):.4f} (+/- {np.std(cv_scores['f2']):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': baseline_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîù En √ñnemli 20 √ñzellik:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=importance_df.head(20), palette='viridis')\n",
    "plt.title(\"Top 20 Most Important Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# T√ºretilen √∂zelliklerin √∂nemi\n",
    "engineered_features = importance_df[importance_df['feature'].str.contains('roll_|diff_')]\n",
    "original_features = importance_df[~importance_df['feature'].str.contains('roll_|diff_')]\n",
    "\n",
    "print(f\"\\nüìà √ñzellik Tipi Analizi:\")\n",
    "print(f\"T√ºretilen √∂zellikler ortalama importance: {engineered_features['importance'].mean():.4f}\")\n",
    "print(f\"Orijinal √∂zellikler ortalama importance: {original_features['importance'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model: Geli≈ütirilmi≈ü Hiperparametreler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daha agresif parametreler\n",
    "final_model = lgb.LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=50,\n",
    "    max_depth=8,\n",
    "    min_child_samples=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,  # L1 regularization\n",
    "    reg_lambda=0.1,  # L2 regularization\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(\"Final model eƒüitiliyor...\")\n",
    "final_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Test tahminleri\n",
    "y_prob_final = final_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_final = (y_prob_final > best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n--- Fƒ∞NAL MODEL SONU√áLARI ---\")\n",
    "print(f\"Threshold: {best_threshold:.2f}\\n\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "\n",
    "# Metrikleri hesapla\n",
    "final_recall = recall_score(y_test, y_pred_final)\n",
    "final_f1 = f1_score(y_test, y_pred_final)\n",
    "final_f2 = fbeta_score(y_test, y_pred_final, beta=2)\n",
    "final_auc = roc_auc_score(y_test, y_prob_final)\n",
    "\n",
    "print(f\"\\nüìä Final Metrikler:\")\n",
    "print(f\"Recall: {final_recall:.4f}\")\n",
    "print(f\"F1 Score: {final_f1:.4f}\")\n",
    "print(f\"F2 Score: {final_f2:.4f}\")\n",
    "print(f\"AUC-ROC: {final_auc:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='RdYlGn')\n",
    "plt.title(\"Final Model: Confusion Matrix\")\n",
    "plt.xlabel(\"Tahmin\")\n",
    "plt.ylabel(\"Ger√ßek\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ve Ayarlarƒ± Kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli kaydet\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "joblib.dump(final_model, os.path.join(models_dir, 'final_lgbm_optimized.pkl'))\n",
    "joblib.dump(X_train.columns.tolist(), os.path.join(models_dir, 'model_features_optimized.pkl'))\n",
    "\n",
    "# Threshold ve diƒüer ayarlarƒ± kaydet\n",
    "model_config = {\n",
    "    'best_threshold': best_threshold,\n",
    "    'feature_names': X_train.columns.tolist(),\n",
    "    'metrics': {\n",
    "        'recall': final_recall,\n",
    "        'f1': final_f1,\n",
    "        'f2': final_f2,\n",
    "        'auc': final_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_config, os.path.join(models_dir, 'model_config_optimized.pkl'))\n",
    "\n",
    "print(f\"\\n‚úÖ Model kaydedildi: {os.path.join(models_dir, 'final_lgbm_optimized.pkl')}\")\n",
    "print(f\"‚úÖ √ñzellik listesi: {os.path.join(models_dir, 'model_features_optimized.pkl')}\")\n",
    "print(f\"‚úÖ Model config: {os.path.join(models_dir, 'model_config_optimized.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √ñzet ve Kar≈üƒ±la≈ütƒ±rma\n",
    "\n",
    "### Yapƒ±lan ƒ∞yile≈ütirmeler:\n",
    "\n",
    "1. ‚úÖ **SMOTE + Undersampling:** Class imbalance √ß√∂z√ºld√º\n",
    "2. ‚úÖ **Threshold Optimizasyonu:** Recall'u maksimize eden threshold bulundu\n",
    "3. ‚úÖ **TimeSeriesSplit:** Temporal validation ile overfitting √∂nlendi\n",
    "4. ‚úÖ **Feature Engineering ƒ∞yile≈ütirmesi:** Daha az, daha kaliteli √∂zellikler\n",
    "5. ‚úÖ **Regularization:** L1/L2 regularization eklendi\n",
    "6. ‚úÖ **Shuffle Kaldƒ±rƒ±ldƒ±:** Temporal integrity korundu\n",
    "\n",
    "### Beklenen ƒ∞yile≈üme:\n",
    "- **√ñnceki Recall:** 0.00 (Model hi√ß arƒ±za yakalayamƒ±yordu)\n",
    "- **Yeni Recall:** > 0.70 bekleniyor (En az %70 arƒ±za yakalanmalƒ±)\n",
    "\n",
    "### Sonraki Adƒ±mlar:\n",
    "1. Modeli production'a almadan √∂nce daha fazla test\n",
    "2. API entegrasyonu\n",
    "3. Dashboard tasarƒ±mƒ±\n",
    "4. Monitoring sistemi kurulumu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
